{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maanpooja/hello-world/blob/master/Corpus_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J3gjycCZE-PI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "\n",
        "import requests\n",
        "import urllib.request\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy6PjNxHE-PL"
      },
      "outputs": [],
      "source": [
        "search_item = input(\"enter your corpus name\")                          #Ask user to enter the corpus name\n",
        "url = (\"https://www.google.co.in/search?q=\" + search_item + \"&num=20\")  #fetch first 20 urls from google search engine\n",
        "print(\"url:\",url)\n",
        "urls1=[]\n",
        "response = requests.get(url)                                            #which contains a server's response to an HTTP request.\n",
        "print(\"Response:\",response)\n",
        "soup = BeautifulSoup(response.text,\"html.parser\")                       #contain Html Code of all the urls\n",
        "print(\"soup:\", soup)\n",
        "for item in soup.find_all(\"href\"):                                      #scraping the data from cite tag which only contain url\n",
        "    if \"youtube\" in item.text:                                         #remove youtube url from all urls\n",
        "        continue\n",
        "    elif \"twitter\" in item.text:                                        #remove twitter url from all urls\n",
        "        continue\n",
        "    elif \"https://\" in item.text:                                      # remove non urls form of data\n",
        "        urls1.append(item.text)\n",
        "    else:\n",
        "        continue                                                         #else continue\n",
        "\n",
        "print(urls1)                                                            #print all the cleaned urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Sw5CT6qdE-PM"
      },
      "outputs": [],
      "source": [
        "data = \"\"                                                                    #created variable to store complete extract/scrap data\n",
        "document = []                                                                # created a list of document\n",
        "count = 0                                                                    #to count number of urls or document\n",
        "with open(\"/content/\" + search_item + \".txt\",'w+') as MyFile:    #create a text file on the desktop\n",
        "    for link in urls1:                                                        # scrap data one by one from each url\n",
        "        url = link\n",
        "        count = count + 1\n",
        "        docs = ''\n",
        "        try:                                                                 #help to remove error link which not allow\n",
        "                                                                             #  to access the link or give http errors\n",
        "            page = requests.get(url)\n",
        "            soup = BeautifulSoup(page.content,'html.parser')                   #get Html code of each of the url\n",
        "            for p in soup.find_all('p'):                                     #get data only from the p tag in html code #Datacleaning\n",
        "                data = data + p.text + \"\\n\\n\"\n",
        "                docs = docs + p.text + \"\\n\\n\"\n",
        "                MyFile.write(p.text + \"\\n\\n\")\n",
        "        except:                                                               # detect all kind error that has give by interpreter\n",
        "            continue                                                           #make it run the code #let it go\n",
        "        document.append(docs)                                                # divide data into different documents based on different urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7e94A1YE-PM",
        "outputId": "eae2ed4c-1f72-4efb-db55-65d434f30e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(data)                                                   # print data stored in variable  'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l6AhkG7E-PN"
      },
      "outputs": [],
      "source": [
        "print(count)                                                   # print the number of document considered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pURljwPfE-PN"
      },
      "outputs": [],
      "source": [
        "print(document[8])                                              # print the sample of document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbRCSYXLE-PN"
      },
      "outputs": [],
      "source": [
        "with open(\"/home/poojamaan/Desktop/Corpus Generation/\" + search_item + \".txt\",'w+') as MyFile:\n",
        "    data1 = MyFile.read()                                                     # used for reading file data\n",
        "    pattern = re.compile(r'(\\[|\\()\\w*(\\)|\\])')                                # pattern to remove square bracket\n",
        "    MyFile.write(pattern.sub('',data)+\"\\n\\n\")                                 # writing cleaned data in file\n",
        "    data = (pattern.sub('',data1)+\"\\n\\n\")                                     # writing cleaned data in data variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4LSMx-RE-PO"
      },
      "outputs": [],
      "source": [
        "# with open(\"/home/poojamaan/Desktop/Corpus Generation/\" + search_item + \".txt\",'r') as MyFile:\n",
        "#     data1 = MyFile.read()\n",
        "#     pattern = re.compile(r'(\\[|\\()\\w*(\\)|\\])')\n",
        "#     data = (pattern.sub('',data1)+\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU-z3m8PE-PO"
      },
      "outputs": [],
      "source": [
        "print(data)                                                                # print global variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oq7EwvHE-PO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')                                       # create a list of stopwords\n",
        "CleanToken = set()                                                          # made a set to remove all the repeat tokens\n",
        "with open(\"/home/poojamaan/Desktop/Corpus Generation/\" + search_item + \".txt\",'r+') as MyFile:\n",
        "    Data = MyFile.read()\n",
        "#     print(word_tokenize(Data))\n",
        "    allToken = word_tokenize(Data)                                             # performing word tokenization\n",
        "    print(len(allToken))\n",
        "\n",
        "    for token in allToken:                                                    # rem0ving unnecesary tokens like ',' '.' '[' & ']'\n",
        "        pattern = re.compile(r'(\\.|\\\"|\\,|\\:|\\;|\\'s|\\'|\\\"\"|\\?|\\(|\\)|\\[|\\]|\\_|\\-|\\&|\\@)')\n",
        "        report = pattern.sub('',token)                                       # using re to convert unnecessary token into empty string\n",
        "        if report == '':                                                     # removing empty string\n",
        "            continue\n",
        "        else:\n",
        "            if report.lower() not in stoplist:                                # removing stopwords\n",
        "                if re.search('([a-z|A-Z|1-9|\\[|\\]])', report):                 # keeping only English words\n",
        "                     CleanToken.add(report)\n",
        "    print(CleanToken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cre0EXuuE-PP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "with open(\"/home/poojamaan/Desktop/Corpus Generation/\" + search_item + \".txt\",'r+') as MyFile:\n",
        "    Data = MyFile.read()\n",
        "    #print(word_tokenize(tokenData))                                     # sentence tokenization\n",
        "    allToken = sent_tokenize(Data)\n",
        "    print(len(allToken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ff5dw8mE-PP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "tagged = nltk.pos_tag(CleanToken)\n",
        "NN = []                                                                  # list of nouns\n",
        "VRB = []                                                                  # list of verbs\n",
        "JJ = []                                                                     # list of adjectives\n",
        "for pos in tagged:\n",
        "    if (pos[1] == 'NN'or pos[1] == 'NNP' or pos[1] == 'NNS'):\n",
        "        NN.append(pos[0].lower())\n",
        "    elif \"V\" in pos[1]:\n",
        "        VRB.append(pos[0].lower())\n",
        "    elif pos[1] == 'JJ':\n",
        "        JJ.append(pos[0].lower())\n",
        "#elif (pos[1] == 'VBD'or pos[1] == 'VB' or pos[1] == 'VBZ' or pos[1] == 'VBP')\n",
        "\n",
        "print(\"NOUNS:\\n\"+ str(NN) + \"\\n\\n\\n\")\n",
        "print(\"VERBS:\\n \"+ str(VRB) + \"\\n\\n\\n\")\n",
        "print(\"ADJECTIVES:\\n \"+ str(JJ) + \"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LymaZfzIE-PP"
      },
      "outputs": [],
      "source": [
        "import fileinput                                                      # package to take input from the user\n",
        "key = input(\"Enter the word that you want to search : \")\n",
        "key = key.lower()                                                     # converting user input into lowercase\n",
        "count = 0                                                             # to print document number where the word is present\n",
        "for i in document:                                                    # taking data from each document to compare key word\n",
        "    count = count + 1                                                 # count document number\n",
        "    content = i                                                        # assining document content\n",
        "    list_of_sentences = content.split(\"\\n\\n\")                        # making list of paragraph from document\n",
        "    # print(list_of_sentences)\n",
        "    for sentence in list_of_sentences:                                # comparing key word with each paragraph\n",
        "    #     print(sentence)\n",
        "        words = sentence.split(\" \")                                    # making list of word from each paragraph\n",
        "    #     print(words)\n",
        "        for word in words:                                             # taking each word\n",
        "            word = word.lower()                                        # converting word in lowercase\n",
        "            if word == key:                                           # comparing each word with key word\n",
        "                print(\"document :\",count)                             # printing respected document number for that word\n",
        "                cross = sentence.split(\" \")\n",
        "                for data in cross:\n",
        "                    data = data.lower()\n",
        "                    if data == key:\n",
        "                        print(\"\\033[1;34;41m\"+ data,end =\" \")        # printing highlighted key word\n",
        "                    else :\n",
        "                        print(\"\\033[1;32;m\"+ data,end =\" \")             # printing other words\n",
        "\n",
        "                print(\"\\n\")\n",
        "#                 print(\"document :\",count)\n",
        "#                 print(sentence)\n",
        "                break                                                 # changing paragraph if once the key word is found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftVAtgfhE-PP"
      },
      "outputs": [],
      "source": [
        "with open(\"/home/poojamaan/Desktop/Corpus Generation/\" + search_item + \".txt\",'w+') as MyFile:\n",
        "    pattern3 = re.compile(r'(\\d*\\s*(January| February| March|April|May|June|July|August|September|October|November|December)\\s*\\d*)')\n",
        "    MyFile.write(pattern3.sub('DATE',data))                                # replacing date format with string DATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN7GNodnE-PP"
      },
      "outputs": [],
      "source": [
        "with open(\"/home/poojamaan/Desktop/Corpus Generation/\" + search_item + \".txt\",'w+') as MyFile:\n",
        "    pattern3 = re.compile(r'(\\d\\d\\d\\d)')\n",
        "    MyFile.write(pattern3.sub('YEAR',data))                              # replacing year format with string YEAR"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}